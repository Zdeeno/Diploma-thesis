\section{Lidar data processing}

To detect individual bricks would be handy to extract straight lines from lidar data. Several methods can be used to achieve this goal. One of the most popular algorithms for lines extraction is currently split and merge algorithm. Initially was this algorithm proposed for image segmentation by Horowitz and Pavlidis \cite{horowitz1974}. Simple version of this algorithm for pointcloud processing is described in algorthm \ref{alg:segmentation}. There are many implementations of this algorithm which differs mainly in a way how they compute some particular steps of the algorithm. For example just the method of fitting a line to cluster can vary a lot. Very often is used the least squares method, but as simple method as connecting endpoints of cluster could be used. When the latter method is applied, algorithm is usually refered as Iterative End Point Fit (IEPF) \cite{siadat1997}. For the cluster creation are the points iterated in each layer one by one. When the distance of subsequent points is too high we split the cluster. Every cluster is then further recursively split based on the most distant point from the fitted line. In a comparison to other line extraction algorithms is the split and merge algorithm one of the best performing in terms of precision and computational complexity \cite{nguyen2006}.
\begin{algorithm}[]
 \KwData{pointcloud}
 \KwResult{line\_segments}
initialize constants C, S\;
  clusters = find\_clusters(pointcloud, C)\;
\While{clusters is not empty}{
	cluster = clusters.pop()\;
	line = fit\_line(cluster)\;
	point = most\_distant\_point(cluster, line)\;
	\eIf{distance(point, line) $>$ S}{
	   c1, c2 = split\_cluster(cluster, point)\;
		clusters.push\_back(c1, c2)\;
	}{
		line\_segments.push\_back(cluster[start], cluster[end])\;	
	}
}
merge\_colinear(line\_segments)\;
 
 \caption{Lidar data segmentation using split and merge algorithm. C is clustering distance and S is splitting distance.}
 \label{alg:segmentation}
\end{algorithm}


\section{EM algorithm}
Expectation-maximization (EM) algorithm is iterative process which can find parameters of certain statistical model based on incomplete data. One of the most used statistical description for the EM algorithm is the Gaussian mixture model. This model is particularly useful because it emerges in many real world situations and it is easy to maximize. As the name of algorithm suggest it repeats expectation and maximization step. Each iteration of the algorithm should improve the likelihood of the model until the terminating criterion is met. Termination criterion could be simply number of iterations or the algorithm can be stopped when the model is not improving anymore. Although we are discussing mainly the Gaussian distribution, EM algorithm can be also used for other distributions from exponential family
 \cite{dempster1977}. 
 
 \subsection{Maximization}
For maximization step is used maximal likelihood estimate weighted by $\alpha$ from expectation step. For parameters of Gaussian distribution $\mathcal{N}(\mu, \sigma)$ and number of samples $N$ looks maximization as follows:
\begin{equation}
\mu = \frac{1}{N} \sum_{n = 1}^{N} \alpha_n x_n \\
\end{equation}
\begin{equation}
\sigma = \frac{1}{N} \sum_{n = 1}^{N}\alpha_n (x_n - \mu)^2.
\end{equation}

The most important assumption for the usage of the algorithm is that its likelihood with respect to estimated parameter must be concave. This can be easily proved by computing the second derivative of likelihood function. For example for mean value $\mu$ it is easy to show that second derivative of likelihood is always negative.
\begin{align}
\mathcal{L} &= \prod_{n=1}^N \mathcal{N}(x_n, \mu, \sigma) \\
\frac{\partial \log \mathcal{L}}{\partial \mu} &= \frac{1}{\sigma^2} \sum_{n = 1}^{N} (x_n - \mu) \\
\frac{\partial^2 \log \mathcal{L}}{\partial \mu^2} &= \frac{-N}{\sigma^2}.
\end{align}

\subsection{Expectation}
The expectation step is done simply by evaluating probability density function of Gaussian distribution with the parameters from maximization step.
\begin{equation}
\alpha_n = \mathcal{N}(x_n, \mu, \sigma),
\end{equation}
where n is index of the data sample. If the prior probabilities of observed random variable are known, it is also possible to exploit the Bayes theorem to compute $\alpha$.

\subsection{Algorithm}
How to implement general version of EM algorithm on sampled data is shown in algorithm \ref{alg:em}. All important calculations for Gaussian distribution are described in previous subsections.It is not clear where to start iterating. It is possible to start both with the expectation and with the maximization step, but both parts are dependent on the result of the other one. Here we start with the maximization step so during the initialization we set $\alpha_n = 1$. If some prior information about parameters of the model is available, they can be set during the initialization and the algorithm can be started with expectation step. This informed initialization can highly reduce the number of iterations and sometimes even an outcome of the algorithm.
\begin{algorithm}[]
 \KwData{x}
 \KwResult{parameters $\theta$}
 set all $\alpha_n$ = 1\;
\While{not stopping\_criterion}{
	$\theta$ = maximization(x, $\alpha$)\;
	$\alpha$ = expectation(x, $\theta$)\;
} 
 \caption{Pseudocode shows how to implement the EM algorithm. x is the observed data.}
 \label{alg:em}
\end{algorithm}


\section{RANSAC}
Random sample consensus (RANSAC) is an iterative method which can estimate parameters of hypothesis given the data. It was first presented by Fischler \cite{fischler1981} with application in scene and image analysis, but it can be used for fitting arbitrary hypothesis. The biggest advantage of this algorithm is its robustness to outliers. Major drawback of this method is very high time complexity when fitting hypothesis to noisy data with large number of samples. Whole iterative process is described in algorithm \ref{alg:ransac}.
\begin{algorithm}[]
 \KwData{x}
 \KwResult{best parameters $\theta^*$}
 initialize $\theta, \theta^*, C, C^*$\;
\While{not stopping\_criterion}{
	samples = draw\_samples(x)\;
	$\theta$ = find\_parameters(samples)\;
	$C$ = compute\_cost(x, $\theta$)\;
	\If{$C > C^*$}{
		$C^* = C$\;
		$\theta^* = \theta$	\;
	}
} 
 \caption{Pseudocode shows how to implement the RANSAC algorithm. x is the observed data, C is the maximized cost and $\theta$ are the parameters of the hypothesis.}
 \label{alg:ransac}
\end{algorithm}

Number of drawn samples in \textbf{draw\_samples} must be equal or higher than the number of degrees of freedom of the hypothesis. After drawing the samples method \textbf{find\_parameters} assigns the correspondences between sampled data and the hypothesis. The correspondences are used to obtain the parameters of the hypothesis. Then is the algorithm evaluating quality of the hypothesis by applying the hypothesis to whole dataset. This can be done by arbitrary cost function. Common practice is to define some metrics in our domain and use a threshold value to obtain the number of samples which fits the hypothesis. These samples are often referred as inliers. Stopping criterion is usually met when the probability of sampling better hypothesis is lower than a specified threshold. This section describes just the basic version of the algorithm. Many improvements to RANSAC algorithm was proposed since 1981 such as \cite{chum2003} or \cite{chum2008}.

\subsection{Tentative Correspondences}
The tentative correspondences can help us to choose better samples from the data to generate the better hypothesis. It is necessary to define some function witch measure the similarity between data and  hypothesis. The data which has higher similarity to the hypothesis are then chosen with higher probability. It is also possible to completely ban correspondences with low similarity. Given the typical application in scene analysis is similarity usually computed by comparing keypoint descriptors.

\section{Lidar to camera registration}
As can be seen in the figure \ref{fig:brickdef} (where the bricks are defined) besides the dimensions another important feature of the bricks is their color. Although, the color manifests itself little bit in reflectivity of the surface which can be detected by lidar, it is not possible reliably distinguish the colors using only the lidar sensor. Until there is a gap between the individual bricks, it is possible to detect brick using just the spatial data. Ideally the robot should be stacking bricks next to each other without any significant gap. Without any information about the color is then impossible to decide whether is the robot detecting one large brick or several small bricks put together. So for this part of detection is necessary to color the pointcloud. This can be done by using the image from Intel RealSense camera and projecting 3D lidar pointcloud to the camera plane. For this purpose is used the pinhole camera model. To describe such a model is used intrinsic camera matrix $K$ which consists of intristic camera parameters \cite{hartley2017}
\begin{equation}
K = \begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix},
\end{equation}
where $f_x, f_y$ are focal lengths and $c_x, c_y$ stands for optical center of the camera.

Firstly is necessary to transform whole pointcloud from lidar coordinate frame to camera frame. For this purpose are used so called extrinsic camera parameters, which describes where in lidar coordinate frame is camera placed. This is more discussed in the next section of the thesis. Secondly we can use the camera intrinsic parameters to calculate the projection. Note that it is needed to work in homogenous 2D coordinate system.
\begin{equation}
\begin{bmatrix}
u\\
v\\
1
\end{bmatrix}
= K \begin{bmatrix}
x\\
y\\
z
\end{bmatrix}.
\end{equation}
Using this matrix equality is possible to decide which coordinates $u,v$ on image plane corresponds to 3D point $x,y,z$ from pointcloud and assign color of a certain pixel to the point. However this works only for the simplest pinhole camera model without any distortion of image. If the lens has non-negligible distortion, this distortion must be included in the camera model. For description of distortion are used the distortion coefficients.

\section{Global model and transformations}
One of the goals of this thesis is develop a global model which can efficiently store and update the positions of interest points. This global model is in the map coordinate frame. The localization of the robot is absolutely essential for precise global model. Every detection must be transformed from the coordinate frame of the sensor to coordinate frame of the map. Transformation can be easily done using matrix multiplication:
\begin{equation}
\begin{bmatrix}
x^\prime \\
y^\prime \\
z^\prime 
\end{bmatrix}
=
R \begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
+ t,
\end{equation}
where $R$ is $3\times3$ the rotation matrix and $t$ is a $1\times3$ translation vector between coordinate frames. Similarly can be the transformation done in homogenous coordinates by merging translation and rotation into one matrix $T$:
\begin{equation}
\begin{bmatrix}
x^\prime \\
y^\prime \\
z^\prime \\
1
\end{bmatrix}
=
\begin{bmatrix}
 & & & \\
 & R & & t \\
 & & & \\
 0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}.
\end{equation}

Different framework for computing the transformations is using a quaternions. The quaternions are currently the standard for transformations in computer graphics and robotics. The biggest advantage of quaternions is that they are more efficient and does not suffer from gymbal lock and ambiguity of rotation. Arbitrary rotation and scaling can be expressed as quadruple of numbers in quaternion framework. The rotation between coordinate frames $B \to A$ is computed with quaternions as:
\begin{equation}
q_A = q_T q_B q_T^*,
\end{equation}
where $q_A$ is quaternion in coordinate frame $A$, $q_B$ is quaternion in coordinate frame $B$, $q_T$ is quaternion representing the transformation between these coordinate frames and $q_T^*$ is its conjugate. There is available a library within the ROS which can handle all these transformations in different forms \cite{tf}. 

\subsection{Symbolic map}
When are all detections transformed into the map frame we can add them into a symbolic map. The symbolic map is storing the positions of all interest points and makes up the global model of the arena. Every object added to symbolic map has a float number which indicates confidence of detection. When there is a new detection within certain range from an object already stored in the symbolic map, new object is not added but only the confidence is increased. This approach creates a clusters of interest points of different types. All interest points can be polled from symbolic map and robot can make decisions based on confidence of such an interest point. (CITACE)